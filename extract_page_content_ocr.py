# pip install playwright easyocr pillow
# For first use, run: playwright install chromium

import csv
import time
import json
import io
from datetime import datetime
from urllib.parse import urljoin, urlparse
from PIL import Image  # pyright: ignore[reportMissingImports]
import easyocr  # pyright: ignore[reportMissingImports]

# Delay Playwright import to avoid error if not installed
try:
    from playwright.sync_api import sync_playwright
except ImportError:
    print("Please install playwright: pip install playwright")
    print("Then run: playwright install chromium")
    exit(1)


def extract_text_from_screenshot(screenshot_bytes: bytes, reader: easyocr.Reader) -> str:
    """
    Use OCR to extract text from screenshot image bytes.
    """
    # Convert bytes to PIL Image
    image = Image.open(io.BytesIO(screenshot_bytes))
    
    # Use easyocr to recognize text from the image
    results = reader.readtext(screenshot_bytes)
    
    # Extract all recognized text with sufficient confidence
    text_lines = []
    for (bbox, text, confidence) in results:
        if confidence > 0.3:  # Only keep results with confidence > 0.3
            text_lines.append(text.strip())
    
    # Merge text, remove successive duplicate lines
    clean_lines = []
    prev_line = None
    for line in text_lines:
        if line and line != prev_line:
            clean_lines.append(line)
            prev_line = line
    
    return '\n'.join(clean_lines)


def extract_page_with_ocr(page, url: str, reader: easyocr.Reader) -> dict:
    """
    Visit the web page and extract content using screenshot + OCR.
    Returns a dict with title, meta description, OCR content, and status.
    """
    try:
        # Visit the page
        page.goto(url, timeout=30000, wait_until='networkidle')
        
        # Wait for the page to load fully
        time.sleep(2)
        
        # Get the page title
        title = page.title() or ""
        
        # Get meta description
        description = ""
        try:
            meta_desc = page.query_selector('meta[name="description"]')
            if meta_desc:
                description = meta_desc.get_attribute('content') or ""
        except Exception:
            pass
        
        # Take a full-page screenshot
        screenshot_bytes = page.screenshot(full_page=True)
        
        # Extract text via OCR
        ocr_content = extract_text_from_screenshot(screenshot_bytes, reader)
        
        return {
            'title': title.strip(),
            'description': description.strip(),
            'content': ocr_content,
            'status': 'success'
        }
        
    except Exception as e:
        return {
            'title': '',
            'description': '',
            'content': '',
            'status': f'error: {str(e)[:100]}'
        }


def find_subpages_playwright(page, base_url: str) -> set:
    """
    Use Playwright to find subpages of the given base URL.
    Only includes links on the same domain.
    """
    subpages = set()
    base_parsed = urlparse(base_url)
    
    try:
        links = page.query_selector_all('a[href]')
        for link in links:
            href = link.get_attribute('href')
            if href:
                href = href.strip()
                full_url = urljoin(base_url, href)
                parsed = urlparse(full_url)
                
                # Only keep links within the same domain and protocol
                if (parsed.scheme, parsed.netloc) == (base_parsed.scheme, base_parsed.netloc):
                    if full_url != base_url and full_url.startswith(base_url):
                        url_no_query = full_url.split('#')[0].split('?')[0]
                        subpages.add(url_no_query)
    except Exception as e:
        print(f"  âš ï¸ Error while finding subpages: {e}")
    
    return subpages


def process_urls_with_ocr(input_file: str, output_file: str = None, 
                          languages: list = None, crawl_subpages: bool = True):
    """
    Process a list of URLs, extracting page content using screenshot + OCR.
    
    Args:
        input_file: File path with input URLs (one per line)
        output_file: Output CSV file path (optional, autogenerated if not supplied)
        languages: List of OCR recognition languages, default ['ch_sim', 'en']
        crawl_subpages: Whether to crawl subpages on the same domain (default True)
    Returns:
        output_file name (CSV)
    """
    if languages is None:
        languages = ['ch_sim', 'en']
    
    # Initialize OCR Reader
    print("ðŸ”„ Initializing OCR engine (models may be downloaded on first run, please wait)...")
    reader = easyocr.Reader(languages, gpu=True)
    print("âœ… OCR engine initialized")
    
    # Read raw URLs from the input file
    with open(input_file, 'r', encoding='utf-8') as f:
        raw_urls = [line.strip() for line in f if line.strip()]
    
    # Preprocess for duplicates (ignore query and fragment)
    all_urls_set = set()
    for u in raw_urls:
        all_urls_set.add(u.split('#')[0].split('?')[0])
    main_urls = list(all_urls_set)
    
    print(f"ðŸ“‹ {len(main_urls)} main URLs to process")
    print("=" * 60)
    
    visited_urls = set()
    to_process_urls = []
    for url in main_urls:
        url_trimmed = url.split('#')[0].split('?')[0]
        if url_trimmed not in to_process_urls:
            to_process_urls.append(url_trimmed)
    
    # Prepare CSV output
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    if output_file is None:
        output_file = f"page_contents_ocr_{timestamp}.csv"
    
    with sync_playwright() as p:
        print("ðŸŒ Launching browser...")
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        )
        page = context.new_page()
        print("âœ… Browser launched")
        
        with open(output_file, 'w', newline='', encoding='utf-8-sig') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow(['URL', 'Title', 'Description', 'OCR_Content', 'Status'])
            
            success_count = 0
            fail_count = 0
            processing_idx = 1
            
            while to_process_urls:
                url = to_process_urls.pop(0)
                url_trimmed = url.split('#')[0].split('?')[0]
                
                if url_trimmed in visited_urls:
                    continue
                
                print(f"\n[{processing_idx}] Processing: {url_trimmed}")
                processing_idx += 1
                
                # Extract content
                result = extract_page_with_ocr(page, url_trimmed, reader)
                
                if result['status'] == 'success':
                    # Stringify extracted content for spreadsheet safety
                    stringified_content = json.dumps(result['content'], ensure_ascii=False)
                    writer.writerow([
                        url_trimmed,
                        result['title'],
                        result['description'],
                        stringified_content,
                        result['status']
                    ])
                    success_count += 1
                    print(f"  âœ… Success - Title: {result['title'][:50] if result['title'] else '(No Title)'}...")
                    print(f"  ðŸ“ OCR extracted {len(result['content'])} characters")
                    
                    # Crawl subpages if enabled
                    if crawl_subpages:
                        subpages = find_subpages_playwright(page, url_trimmed)
                        new_subs = [u for u in subpages if u not in visited_urls and u not in to_process_urls]
                        if new_subs:
                            print(f"  ðŸ” Found {len(new_subs)} subpages, adding to processing queue")
                            to_process_urls.extend(new_subs)
                else:
                    writer.writerow([url_trimmed, '', '', '', result['status']])
                    fail_count += 1
                    print(f"  âŒ {result['status']}")
                
                visited_urls.add(url_trimmed)
                
                # Progress display
                total = len(visited_urls) + len(to_process_urls)
                progress = len(visited_urls) / total * 100 if total > 0 else 100
                print(f"  ðŸ“Š Progress: {progress:.1f}% (Processed: {len(visited_urls)}; Pending: {len(to_process_urls)})")
                
                # Polite delay between requests
                if to_process_urls:
                    time.sleep(1)
        
        browser.close()
    
    print("\n" + "=" * 60)
    print("âœ… Processing complete!")
    print(f"   Success: {success_count}")
    print(f"   Failed: {fail_count}")
    print(f"   Output file: {output_file}")
    
    return output_file


def process_single_url_with_ocr(url: str, languages: list = None, 
                                save_screenshot: bool = False) -> dict:
    """
    Process a single URL, extracting page content using screenshot + OCR.
    
    Args:
        url: URL to process
        languages: List of OCR recognition languages
        save_screenshot: Whether to save the screenshot as a PNG
    
    Returns:
        Dictionary with keys: url, title, content, status
    """
    if languages is None:
        languages = ['ch_sim', 'en']
    
    print("ðŸ”„ Initializing OCR engine...")
    reader = easyocr.Reader(languages, gpu=True)
    
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        )
        page = context.new_page()
        
        try:
            page.goto(url, timeout=30000, wait_until='networkidle')
            time.sleep(2)
            
            title = page.title() or ""
            
            # Take screenshot
            screenshot_bytes = page.screenshot(full_page=True)
            
            # Optionally save screenshot
            if save_screenshot:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                screenshot_file = f"screenshot_{timestamp}.png"
                with open(screenshot_file, 'wb') as f:
                    f.write(screenshot_bytes)
                print(f"ðŸ“¸ Screenshot saved: {screenshot_file}")
            
            # OCR extraction
            ocr_content = extract_text_from_screenshot(screenshot_bytes, reader)
            
            result = {
                'url': url,
                'title': title,
                'content': ocr_content,
                'status': 'success'
            }
            
        except Exception as e:
            result = {
                'url': url,
                'title': '',
                'content': '',
                'status': f'error: {str(e)}'
            }
        
        browser.close()
    
    return result


if __name__ == "__main__":
    INPUT_FILE = "input_website_list_cleaned.txt"
    
    # Batch process URL list (including subpage crawl)
    process_urls_with_ocr(
        INPUT_FILE,
        languages=['ch_sim', 'en'],  # Supports Simplified Chinese & English
        crawl_subpages=True           # Whether to crawl subpages on the same domain
    )
    
    # Or, usage example for a single URL:
    # result = process_single_url_with_ocr(
    #     "https://example.com",
    #     save_screenshot=True
    # )
    # print(result)
